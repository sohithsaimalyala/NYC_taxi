{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc080774-f41a-4503-a064-c0ac819eeb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reg fetch new batch of features and compute predictions and save to feature store\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bd038bf-8550-420b-9ec2-308198fca80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46b8ae0d-58dd-4772-8931-25ff5fccb749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "import src.config as config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26612685-265e-4415-a149-5751be8f994d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-06 11:16:05,258 INFO: Initializing external client\n",
      "2025-03-06 11:16:05,258 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2025-03-06 11:16:06,200 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1214708\n",
      "Fetching data from 2025-02-04 15:16:05.255400+00:00 to 2025-03-06 15:16:05.255400+00:00\n",
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (4.34s) \n",
      "\n",
      "Processed 252 locations with sufficient data\n",
      "Error: too many values to unpack (expected 2)\n",
      "\n",
      "Debug Info:\n",
      "Processed data shape: (169848, 3)\n",
      "Locations processed: 252\n"
     ]
    }
   ],
   "source": [
    "from src.inference import get_feature_store, load_model_from_registry, get_model_predictions\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import src.config as config\n",
    "import lightgbm as lgb\n",
    "\n",
    "try:\n",
    "    # Step 1: Setup and get feature store data\n",
    "    current_date = pd.Timestamp.now(tz='Etc/UTC')\n",
    "    feature_store = get_feature_store()\n",
    "    \n",
    "    fetch_data_to = current_date - timedelta(hours=1)\n",
    "    fetch_data_from = fetch_data_to - timedelta(days=30)\n",
    "    \n",
    "    print(f\"Fetching data from {fetch_data_from} to {fetch_data_to}\")\n",
    "    \n",
    "    feature_view = feature_store.get_feature_view(\n",
    "        name=config.FEATURE_VIEW_NAME,\n",
    "        version=config.FEATURE_VIEW_VERSION\n",
    "    )\n",
    "    \n",
    "    # Get and prepare initial data\n",
    "    ts_data = feature_view.get_batch_data(\n",
    "        start_time=fetch_data_from,\n",
    "        end_time=fetch_data_to\n",
    "    )\n",
    "    \n",
    "    # Step 2: Prepare continuous time series data\n",
    "    ts_data['pickup_hour'] = ts_data['pickup_hour'].dt.tz_localize(None)\n",
    "    \n",
    "    # Create full date range\n",
    "    date_range = pd.date_range(\n",
    "        start=ts_data.pickup_hour.min(),\n",
    "        end=ts_data.pickup_hour.max(),\n",
    "        freq='H'\n",
    "    )\n",
    "    \n",
    "    # Process each location to ensure continuous data\n",
    "    processed_locations = []\n",
    "    \n",
    "    for location_id in ts_data.pickup_location_id.unique():\n",
    "        loc_data = ts_data[ts_data.pickup_location_id == location_id].copy()\n",
    "        \n",
    "        # Create continuous series for location\n",
    "        location_series = pd.DataFrame({\n",
    "            'pickup_hour': date_range,\n",
    "            'pickup_location_id': location_id\n",
    "        })\n",
    "        \n",
    "        # Merge with actual data\n",
    "        location_series = location_series.merge(\n",
    "            loc_data[['pickup_hour', 'rides']], \n",
    "            on='pickup_hour', \n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # Fill missing values with 0\n",
    "        location_series['rides'] = location_series['rides'].fillna(0)\n",
    "        \n",
    "        # Only keep locations with sufficient data\n",
    "        if len(location_series) >= 672:  # 28 days * 24 hours\n",
    "            processed_locations.append(location_series)\n",
    "    \n",
    "    # Combine processed locations\n",
    "    if not processed_locations:\n",
    "        raise ValueError(\"No locations with sufficient data found\")\n",
    "    \n",
    "    ts_data_processed = pd.concat(processed_locations)\n",
    "    print(f\"\\nProcessed {len(processed_locations)} locations with sufficient data\")\n",
    "    \n",
    "    # Step 3: Generate features\n",
    "    from src.data_utils import transform_ts_data_info_features\n",
    "    features, _ = transform_ts_data_info_features(\n",
    "        ts_data,\n",
    "        window_size=504,  # 21 days\n",
    "        step_size=24\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Add missing column\n",
    "    features['rides_t-672'] = 0\n",
    "    \n",
    "    # Step 4: Load and prepare model\n",
    "    model = load_model_from_registry()\n",
    "    if isinstance(model, lgb.Booster):\n",
    "        model.params['predict_disable_shape_check'] = True\n",
    "    elif hasattr(model, 'steps') and isinstance(model.steps[-1][1], lgb.LGBMRegressor):\n",
    "        model.steps[-1][1].set_params(predict_disable_shape_check=True)\n",
    "    \n",
    "    # Step 5: Generate predictions\n",
    "    predictions = get_model_predictions(model, features)\n",
    "    \n",
    "    if predictions is not None and not predictions.empty:\n",
    "        predictions['pickup_hour'] = current_date.ceil('h')\n",
    "        \n",
    "        # Save to feature store\n",
    "        feature_group = feature_store.get_or_create_feature_group(\n",
    "            name=config.FEATURE_GROUP_MODEL_PREDICTION,\n",
    "            version=1,\n",
    "            description=\"Predictions from LGBM Model\",\n",
    "            primary_key=['pickup_location_id', 'pickup_hour'],\n",
    "            event_time='pickup_hour',\n",
    "        )\n",
    "        \n",
    "        feature_group.insert(predictions, write_options={\"wait_for_job\": False})\n",
    "        \n",
    "        print(f\"\\nSaved {len(predictions)} predictions to feature store\")\n",
    "        print(\"\\nTop 10 locations by predicted demand:\")\n",
    "        print(predictions.sort_values('predicted_demand', ascending=False)[\n",
    "            ['pickup_location_id', 'predicted_demand']\n",
    "        ].head(10))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    print(\"\\nDebug Info:\")\n",
    "    if 'ts_data_processed' in locals():\n",
    "        print(f\"Processed data shape: {ts_data_processed.shape}\")\n",
    "        print(f\"Locations processed: {ts_data_processed.pickup_location_id.nunique()}\")\n",
    "    \n",
    "predictions if 'predictions' in locals() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22253dce-140a-4296-a48b-35c6c7655b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-06 11:16:16,330 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2025-03-06 11:16:16,339 INFO: Initializing external client\n",
      "2025-03-06 11:16:16,340 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2025-03-06 11:16:16,933 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1214708\n",
      "Downloading model artifact (0 dirs, 1 files)... DONE\r"
     ]
    }
   ],
   "source": [
    "from src.inference import load_model_from_registry\n",
    "\n",
    "model = load_model_from_registry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b868f801-03a3-4d51-8e24-88231e664f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data from 2025-02-04 15:16:19.096000+00:00 to 2025-03-06 15:16:19.096000+00:00\n",
      "2025-03-06 11:16:19,096 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2025-03-06 11:16:19,106 INFO: Initializing external client\n",
      "2025-03-06 11:16:19,107 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2025-03-06 11:16:19,819 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1214708\n",
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (5.31s) \n",
      "Data loaded: 169848 records\n",
      "2025-03-06 11:16:31,489 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2025-03-06 11:16:31,496 INFO: Initializing external client\n",
      "2025-03-06 11:16:31,497 INFO: Base URL: https://c.app.hopsworks.ai:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-06 11:16:32,064 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1214708\n",
      "Downloading model artifact (0 dirs, 1 files)... DONE\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "Uploading Dataframe: 100.00% |██████████| Rows 2016/2016 | Elapsed Time: 00:00 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: taxi_hourly_model_prediction_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai:443/p/1214708/jobs/named/taxi_hourly_model_prediction_1_offline_fg_materialization/executions\n",
      "\n",
      "Saved 2016 predictions to feature store\n",
      "\n",
      "Top 10 locations by predicted demand:\n",
      "      pickup_location_id  predicted_demand\n",
      "1207                 161             100.0\n",
      "1402                 186              95.0\n",
      "1900                 249              90.0\n",
      "1749                 230              89.0\n",
      "1063                 142              89.0\n",
      "1800                 237              87.0\n",
      "1794                 236              86.0\n",
      "1201                 161              84.0\n",
      "1206                 161              84.0\n",
      "604                   79              84.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_location_id</th>\n",
       "      <th>predicted_demand</th>\n",
       "      <th>pickup_hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2025-03-06 17:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2025-03-06 17:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2025-03-06 17:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2025-03-06 17:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2025-03-06 17:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>263</td>\n",
       "      <td>40.0</td>\n",
       "      <td>2025-03-06 17:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>263</td>\n",
       "      <td>47.0</td>\n",
       "      <td>2025-03-06 17:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>263</td>\n",
       "      <td>39.0</td>\n",
       "      <td>2025-03-06 17:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>263</td>\n",
       "      <td>41.0</td>\n",
       "      <td>2025-03-06 17:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>263</td>\n",
       "      <td>42.0</td>\n",
       "      <td>2025-03-06 17:00:00+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2016 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pickup_location_id  predicted_demand               pickup_hour\n",
       "0                      2               0.0 2025-03-06 17:00:00+00:00\n",
       "1                      2               0.0 2025-03-06 17:00:00+00:00\n",
       "2                      2               0.0 2025-03-06 17:00:00+00:00\n",
       "3                      2               0.0 2025-03-06 17:00:00+00:00\n",
       "4                      2               0.0 2025-03-06 17:00:00+00:00\n",
       "...                  ...               ...                       ...\n",
       "2011                 263              40.0 2025-03-06 17:00:00+00:00\n",
       "2012                 263              47.0 2025-03-06 17:00:00+00:00\n",
       "2013                 263              39.0 2025-03-06 17:00:00+00:00\n",
       "2014                 263              41.0 2025-03-06 17:00:00+00:00\n",
       "2015                 263              42.0 2025-03-06 17:00:00+00:00\n",
       "\n",
       "[2016 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.inference import get_feature_store, load_model_from_registry, get_model_predictions\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import src.config as config\n",
    "import lightgbm as lgb\n",
    "\n",
    "try:\n",
    "    # Step 1: Setup time window\n",
    "    current_date = pd.Timestamp.now(tz='Etc/UTC')\n",
    "    fetch_data_to = current_date - timedelta(hours=1)\n",
    "    fetch_data_from = fetch_data_to - timedelta(days=30)\n",
    "    \n",
    "    print(f\"Fetching data from {fetch_data_from} to {fetch_data_to}\")\n",
    "    \n",
    "    # Step 2: Get feature store data\n",
    "    feature_store = get_feature_store()\n",
    "    feature_view = feature_store.get_feature_view(\n",
    "        name=config.FEATURE_VIEW_NAME,\n",
    "        version=config.FEATURE_VIEW_VERSION\n",
    "    )\n",
    "    \n",
    "    ts_data = feature_view.get_batch_data(\n",
    "        start_time=fetch_data_from,\n",
    "        end_time=fetch_data_to\n",
    "    )\n",
    "    \n",
    "    ts_data['pickup_hour'] = ts_data['pickup_hour'].dt.tz_localize(None)\n",
    "    ts_data = ts_data.sort_values(['pickup_location_id', 'pickup_hour'])\n",
    "    \n",
    "    print(f\"Data loaded: {len(ts_data)} records\")\n",
    "    \n",
    "    # Step 3: Generate initial features\n",
    "    from src.data_utils import transform_ts_data_info_features_and_target  # or _loop if needed\n",
    "    features = transform_ts_data_info_features(\n",
    "        ts_data,\n",
    "        window_size=504,  # 21 days\n",
    "        step_size=24\n",
    "    )\n",
    "    \n",
    "    # Step 4: Add all required time lag columns\n",
    "    max_lag = 672  # Maximum required lag\n",
    "    current_lags = set(col for col in features.columns if col.startswith('rides_t-'))\n",
    "    \n",
    "    # Add missing time lag columns with zeros\n",
    "    for lag in range(1, max_lag + 1):\n",
    "        col_name = f'rides_t-{lag}'\n",
    "        if col_name not in current_lags:\n",
    "            features[col_name] = 0\n",
    "    \n",
    "    # Step 5: Load and prepare model\n",
    "    model = load_model_from_registry()\n",
    "    \n",
    "    # Modify model parameters to handle shape mismatch\n",
    "    if isinstance(model, lgb.Booster):\n",
    "        model.params['predict_disable_shape_check'] = True\n",
    "    elif hasattr(model, 'steps') and isinstance(model.steps[-1][1], lgb.LGBMRegressor):\n",
    "        model.steps[-1][1].set_params(predict_disable_shape_check=True)\n",
    "    \n",
    "    # Step 6: Generate predictions\n",
    "    predictions = get_model_predictions(model, features)\n",
    "    \n",
    "    if predictions is not None and not predictions.empty:\n",
    "        # Add timestamp and save predictions\n",
    "        predictions['pickup_hour'] = current_date.ceil('h')\n",
    "        \n",
    "        feature_group = feature_store.get_or_create_feature_group(\n",
    "            name=config.FEATURE_GROUP_MODEL_PREDICTION,\n",
    "            version=1,\n",
    "            description=\"Predictions from LGBM Model\",\n",
    "            primary_key=['pickup_location_id', 'pickup_hour'],\n",
    "            event_time='pickup_hour',\n",
    "        )\n",
    "        \n",
    "        feature_group.insert(predictions, write_options={\"wait_for_job\": False})\n",
    "        \n",
    "        print(f\"\\nSaved {len(predictions)} predictions to feature store\")\n",
    "        print(\"\\nTop 10 locations by predicted demand:\")\n",
    "        print(predictions.sort_values('predicted_demand', ascending=False)[\n",
    "            ['pickup_location_id', 'predicted_demand']\n",
    "        ].head(10))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    print(\"\\nDebug Info:\")\n",
    "    if 'features' in locals():\n",
    "        print(f\"Features shape: {features.shape}\")\n",
    "        print(f\"Number of time lag columns: {len([c for c in features.columns if c.startswith('rides_t-')])}\")\n",
    "    \n",
    "# Display predictions\n",
    "predictions if 'predictions' in locals() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "751d63ab-8e98-4087-a3cf-d79bd1bd88e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_location_id</th>\n",
       "      <th>predicted_demand</th>\n",
       "      <th>pickup_hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2025-03-06 17:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2025-03-06 17:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2025-03-06 17:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2025-03-06 17:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2025-03-06 17:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>263</td>\n",
       "      <td>40.0</td>\n",
       "      <td>2025-03-06 17:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>263</td>\n",
       "      <td>47.0</td>\n",
       "      <td>2025-03-06 17:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>263</td>\n",
       "      <td>39.0</td>\n",
       "      <td>2025-03-06 17:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>263</td>\n",
       "      <td>41.0</td>\n",
       "      <td>2025-03-06 17:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>263</td>\n",
       "      <td>42.0</td>\n",
       "      <td>2025-03-06 17:00:00+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2016 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pickup_location_id  predicted_demand               pickup_hour\n",
       "0                      2               0.0 2025-03-06 17:00:00+00:00\n",
       "1                      2               0.0 2025-03-06 17:00:00+00:00\n",
       "2                      2               0.0 2025-03-06 17:00:00+00:00\n",
       "3                      2               0.0 2025-03-06 17:00:00+00:00\n",
       "4                      2               0.0 2025-03-06 17:00:00+00:00\n",
       "...                  ...               ...                       ...\n",
       "2011                 263              40.0 2025-03-06 17:00:00+00:00\n",
       "2012                 263              47.0 2025-03-06 17:00:00+00:00\n",
       "2013                 263              39.0 2025-03-06 17:00:00+00:00\n",
       "2014                 263              41.0 2025-03-06 17:00:00+00:00\n",
       "2015                 263              42.0 2025-03-06 17:00:00+00:00\n",
       "\n",
       "[2016 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[\"pickup_hour\"] = current_date.ceil('h')\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b171dd4-628a-4c46-af00-92cee476f090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-06 11:16:42,575 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2025-03-06 11:16:42,591 INFO: Initializing external client\n",
      "2025-03-06 11:16:42,591 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2025-03-06 11:16:43,206 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1214708\n"
     ]
    }
   ],
   "source": [
    "from src.inference import get_feature_store\n",
    "\n",
    "feature_group = get_feature_store().get_or_create_feature_group(\n",
    "    name=config.FEATURE_GROUP_MODEL_PREDICTION,\n",
    "    version=1,\n",
    "    description=\"Predictions from LGBM Model\",\n",
    "    primary_key=[\"pickup_location_id\", \"pickup_hour\"],\n",
    "    event_time=\"pickup_hour\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ba8f0db-022e-4f8a-ac34-6fd8bb095b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |██████████| Rows 2016/2016 | Elapsed Time: 00:00 | Remaining Time: 00:00\n",
      "UserWarning: Materialization job is already running, aborting new execution.Please wait for the current execution to finish before triggering a new one.You can check the status of the current execution using `fg.materialization_job.get_state()`.or `fg.materialization_job.get_final_state()` or check it out in the Hopsworks UI.at https://c.app.hopsworks.ai:443/p/1214708/jobs/named/taxi_hourly_model_prediction_1_offline_fg_materialization.\n",
      "Use fg.materialization_job.run(args=-op offline_fg_materialization -path hdfs:///Projects/sp25taxiproject/Resources/jobs/taxi_hourly_model_prediction_1_offline_fg_materialization/config_1741247733057) to trigger the materialization job again.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Job('taxi_hourly_model_prediction_1_offline_fg_materialization', 'SPARK'),\n",
       " None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_group.insert(predictions, write_options={\"wait_for_job\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6108be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sohith",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
